{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyrUypEsYM-t"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re, string, unicodedata\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re,string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "aSpC3vdii9Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T3GJfRnC50tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "wUs-rb5sYXMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/IMDB/IMDB Dataset.csv\",header=0)\n",
        "data"
      ],
      "metadata": {
        "id": "s4Zb5S-jYZ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "LW2Y9CPfYcpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(data.shape[1]):\n",
        "  print(data.iloc[:,i].unique())"
      ],
      "metadata": {
        "id": "3BKxkRiWYktr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(data.shape[1]):\n",
        "  print(data.iloc[:,i].value_counts())"
      ],
      "metadata": {
        "id": "NOB3BzobYnTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "sR07ExrwYsaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "4BVWEQlvYu2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "iWVt3eFjYw1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "67t3IFdHYzTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "data['review']=data['review'].apply(denoise_text)"
      ],
      "metadata": {
        "id": "kuzFaMgtY2Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "data['review']=data['review'].apply(remove_special_characters)"
      ],
      "metadata": {
        "id": "PVfdJKY4ZUEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "#Apply function on review column\n",
        "data['review']=data['review'].apply(simple_stemmer)"
      ],
      "metadata": {
        "id": "BtEBYjM6ZY3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "F3laBxaiaqrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "data['review']=data['review'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "QDgjGdnLaron"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sentiment = [ 1 if each == \"positive\" else 0 for each in data.sentiment]"
      ],
      "metadata": {
        "id": "E81NOv3ylvcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "qAfzaAjabDYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RbhKUV6sbG6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to vectors using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf_train = vectorizer.fit_transform(X_train)\n",
        "tfidf_test = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "cCBIktAl3bIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a logistic regression object\n",
        "lr = LogisticRegression()"
      ],
      "metadata": {
        "id": "7FX77jxz3t2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data\n",
        "lr.fit(tfidf_train, y_train)"
      ],
      "metadata": {
        "id": "6cgAPLAw38QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the sentiment labels for the testing data\n",
        "y_pred = lr.predict(tfidf_test)"
      ],
      "metadata": {
        "id": "pyHwGAD93-7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model\n",
        "acc = accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "1rFf6HFs4Psd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "nuQ2KrYu4SqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and download Word2Vec vectors\n",
        "sentences = [review.split() for review in data['review']]\n",
        "model = Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n",
        "model.save('word2vec.model')"
      ],
      "metadata": {
        "id": "mIuCKJ8pyUoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each word to vector and represent the sentence in vector form using the word embeddings\n",
        "def sentence_vector(sentence,model):\n",
        "    words = sentence.split()\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv.vocab]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros((100,))\n",
        "    return np.mean(word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "wnEYOdrFro8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each sentence in the dataset to a vector using the Word2Vec model\n",
        "word2vec_train = np.array([sentence_vector(sentence, model) for sentence in data['review']])\n",
        "word2vec_test = np.array(data['sentiment'])"
      ],
      "metadata": {
        "id": "QVAgHM4BrL2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(word2vec_train, word2vec_test, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fSa8haIU5Tj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a logistic regression object\n",
        "lr2 = LogisticRegression()"
      ],
      "metadata": {
        "id": "SoewQ8iX5kkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data\n",
        "lr2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "vhtg96_Z5oRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the sentiment labels for the testing data\n",
        "y_pred = lr2.predict(X_test)"
      ],
      "metadata": {
        "id": "GiThJDX-5qzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model\n",
        "acc = accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "duMebX5j5urw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "_bI5GyFX5xqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_vectors(doc):\n",
        "    tfidf_vec = vectorizer.transform([doc])\n",
        "    w2v_vec = sentence_vector(doc, model)\n",
        "    combined_vec = np.concatenate([np.squeeze(tfidf_vec.toarray()), w2v_vec])\n",
        "    return combined_vec"
      ],
      "metadata": {
        "id": "pazDKlUssPod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_train = np.array([combine_vectors(doc) for doc in data['review']])\n",
        "combined_test = np.array(data['sentiment'])"
      ],
      "metadata": {
        "id": "N3fgNuQMO8Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(combined_train, combined_test, test_size=0.2, random_state=56)"
      ],
      "metadata": {
        "id": "CJ7w9Vwp-0rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr3 = LogisticRegression()\n",
        "lr3.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "R2aZzpkgBlRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predlr = lr3.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_predlr)\n",
        "print('Accuracy:', acc)\n",
        "\n",
        "print(\"\\n\\n........................CLASSIFICATION MATRIX.....................\")\n",
        "print(classification_report(y_predlr,y_test))\n",
        "\n",
        "cm=confusion_matrix(y_test, y_predlr, labels=lr3.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=lr3.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYtLI3VgB9NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc2= LinearSVC(penalty='l2',loss='hinge')\n",
        "svc2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NptYesy_g_i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc1pred= svc2.predict(X_test)\n",
        "svcscore = accuracy_score(y_test, svc1pred)\n",
        "print(\"SVC Accuracy: \", round((100*svcscore),2),\"%\")\n",
        "\n",
        "print(\"\\n\\n........................CLASSIFICATION MATRIX.....................\")\n",
        "print(classification_report(svc1pred,y_test))\n",
        "\n",
        "cm=confusion_matrix(y_test, svc1pred, labels=svc2.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=svc2.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cm1_g_dEDiHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgbc=  XGBClassifier()\n",
        "xgbc.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "O8Xk1Dr7ktd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb1pred= xgbc.predict(X_test)\n",
        "xgbscore = accuracy_score(y_test, xgb1pred)\n",
        "print(\"XGB Accuracy: \", round((100*xgbscore),2),\"%\")\n",
        "\n",
        "print(\"\\n\\n........................CLASSIFICATION MATRIX.....................\")\n",
        "print(classification_report(xgb1pred,y_test))\n",
        "\n",
        "cm=confusion_matrix(y_test, xgb1pred, labels=xgbc.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=xgbc.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W4e6lgYbkrL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "metadata": {
        "id": "SVgh5WSL1HOs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}